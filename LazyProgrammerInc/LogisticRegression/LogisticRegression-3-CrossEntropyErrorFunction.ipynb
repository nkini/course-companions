{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression: Squared Error\n",
    "\n",
    "$J = \\sum_n (t_n - y_n)^2$\n",
    "\n",
    "Assumes Gaussian-distributed error, because log(Gaussian) = squared function.\n",
    "\n",
    "Logistic regression error can't be Gaussian, because:\n",
    "- Target is only 0 or 1\n",
    "- Output is only a number between 0 and 1\n",
    "- We want\n",
    "    - $0$ cost if correct, $> 0$ if not correct, and bigger cost wheen more wrong.\n",
    "    \n",
    "    \n",
    "- \"Cross-entropy assumes the same kind of distribution you'd get with a coin toss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy error\n",
    "\n",
    "$J = -{t * log(y) + (1 - t) * log (1 - y)}$ (t = target, y = output of logistic)\n",
    "\n",
    "- Only one of the two terms contribute for a given y.\n",
    "- log y $\\Rightarrow$ number between 0 and -inf, since y is between 0 and 1.\n",
    "- E.g.:\n",
    "    - $t = 1, y = 1 \\Rightarrow 0$\n",
    "    - $t = 0, y = 0 \\Rightarrow 0$\n",
    "    - $t = 1, y = 0.9 \\Rightarrow 0.11$\n",
    "    - $t = 1, y = 0.5 \\Rightarrow 0.69$\n",
    "    - $t = 1, y = 0.1 \\Rightarrow 2.3$\n",
    "- But wait, the $J$ above was for a single example. We want this for all predicted examples:\n",
    "$J = - \\sum_n {t_n * log(y_n) + (1 - t_n) * log (1 - y_n)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 100\n",
    "D = 2\n",
    "\n",
    "# Gaussian centered at 0 (std = 1)\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "# Clever trick: Centering at (-2, -2)\n",
    "X[:50, :] = X[:50, :] - 2 * np.ones((50, D))\n",
    "\n",
    "# Centering at (2, 2)\n",
    "X[50:, :] = X[50:, :] + 2 * np.ones((50, D))\n",
    "\n",
    "# Array of targets\n",
    "# First 50 at 0 and next 50 at 1\n",
    "T = np.array([0]*50 + [1]*50).T \n",
    "\n",
    "ones = np.array([[1] * N]).T\n",
    "Xb = np.concatenate((ones, X), axis=1)\n",
    "w = np.random.randn(D + 1)\n",
    "\n",
    "# Calculate the model output\n",
    "z = Xb.dot(w)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "Y = sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(T, Y):\n",
    "    return - (T * np.log(Y) + (1 - T) * np.log(1 - Y)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331.950104435\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy(T, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331.950104435\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy2(T, Y):\n",
    "    E = 0\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E\n",
    "print(cross_entropy2(T, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0830137837387\n",
      "0.0830137837387\n"
     ]
    }
   ],
   "source": [
    "# Use the closed form solution and see how well it does\n",
    "# (works because both classes are Gaussian distributed and\n",
    "#     variances = 1)\n",
    "# So the wts depend only on the means\n",
    "\n",
    "# Was calculated analytically using the closed form solution\n",
    "w2 = np.array([0, 4, 4])\n",
    "\n",
    "z2 = Xb.dot(w2)\n",
    "Y2 = sigmoid(z2)\n",
    "print(cross_entropy(T, Y2))\n",
    "print(cross_entropy2(T, Y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice how low the error is when using the closed form solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Available approaches:\n",
    "    - Closed form solution:\n",
    "        - Requires data to be Gaussian distributed with equal covariance\n",
    "    - Linear regression approach:\n",
    "        - Recall what we did in Linear Regression:\n",
    "            - Take derivative of cost function (OLS), set to 0, solve for weights\n",
    "            - Can't do this with logistic regression's cross-entropy loss\n",
    "            - You can get the derivative, but not solve for weights (try this)\n",
    "    - Gradient Descent\n",
    "    \n",
    "### Gradient Descent\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td><img src='GradientDescent.png'></td>\n",
    "    <td><img src='GradientDescent2.png'></td>\n",
    "</tr>\n",
    "</table>\n",
    "- Idea: Take small steps in the direction of derviative\n",
    "- Step size is learning rate\n",
    "- e.g. w = -2\n",
    "- w = -2 - 1 * (-1) - 1\n",
    "- Now we're closer to the optimal point (w = 0)\n",
    "- The slope is 0 at the bottom, so no more changes will occur\n",
    "- This works for any objective function, as long as you can calculate the derivative.\n",
    "- How do you choose the learning rate? No scientific method yet. Try different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient Descent for logistic regression\n",
    "\n",
    "$J = - \\sum_{n=1}^{N} t_n * log(y_n) + (1 - t_n) * log (1 - y_n)$\n",
    "\n",
    "Split into 3 derivatives:  \n",
    "$\\frac{\\partial J}{\\partial w_i} = \n",
    "           - \\sum_{n=1}^{N} \\frac{\\partial J}{\\partial y_n}\n",
    "                            \\frac{\\partial y_n}{\\partial a_n}\n",
    "                            \\frac{\\partial a_n}{\\partial w_i}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial y_n} = t_n \\frac{1}{y_n} + (1 - t_n) \\frac{1}{1 - y_n} (-1) = \\frac{t_n}{y_n} - \\frac{(1 - t_n)}{1 - y_n}$\n",
    "\n",
    "$y_n = \\sigma({a_n}) = \\frac{1}{1 + exp(-a_n)}$\n",
    "\n",
    "$ \\frac{\\partial y_n}{\\partial a_n} = \n",
    "            \\frac{-1}{1 + exp(-a_n)} exp(-a_n) (-1) \n",
    "           = \\frac{exp(-a_n)}{(1 + exp(-a_n)^2}\n",
    "           = \\frac{1}{1 + exp(-a_n)} \\frac{exp(-a_n)}{1 + exp(-a_n)}\n",
    "           = y_n (1 - y_n) $\n",
    "           \n",
    "$a_n = w^T x_n = w_0 x_{n0} + w_1 x_{n1} + w_2 x_{n2} + ...$\n",
    "\n",
    "$\\frac{\\partial a_n}{\\partial w_i} = x_{ni}$\n",
    "\n",
    "Put them together:  \n",
    "$ \\frac{\\partial J}{\\partial w_i} =  - \\sum_{n=1}^{N} \\frac{t_n}{y_n} y_n (1 - y_n) x_{ni} -  \\frac{(1 - t_n)}{1 - y_n} y_n (1 - y_n) x_{ni} $\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_i} = \\sum_{n=1}^{N} (y_n - t_n) x_{ni}$\n",
    "\n",
    "Vectorize:  \n",
    "$\\frac{\\partial J}{\\partial w} = \\sum_{n=1}^{N} (y_n - t_n) x_n = X^T(Y - T)$\n",
    "\n",
    "Bias Term:  \n",
    "$\\frac{\\partial J}{\\partial w_0} = \\sum_{n=1}{N} (y_n - t_n) x_{n0} = \\sum_{n=1}{N} (y_n - t_n) $  \n",
    "(because the $x_0$s are 1 (weights change, $x$ doesn't))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.9741949381\n",
      "1.30758366212\n",
      "0.722971041569\n",
      "0.537385169235\n",
      "0.455394576041\n",
      "0.408661150934\n",
      "0.376405876741\n",
      "0.351251524457\n",
      "0.330261143947\n",
      "0.312103241135\n",
      "Final w: [-3.07998631  2.86749167  5.43367456]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 100\n",
    "D = 2\n",
    "\n",
    "# Gaussian centered at 0 (std = 1)\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "# Clever trick: Centering at (-2, -2)\n",
    "X[:50, :] = X[:50, :] - 2 * np.ones((50, D))\n",
    "\n",
    "# Centering at (2, 2)\n",
    "X[50:, :] = X[50:, :] + 2 * np.ones((50, D))\n",
    "\n",
    "# Array of targets\n",
    "# First 50 at 0 and next 50 at 1\n",
    "T = np.array([0]*50 + [1]*50).T \n",
    "\n",
    "ones = np.array([[1] * N]).T\n",
    "Xb = np.concatenate((ones, X), axis=1)\n",
    "w = np.random.randn(D + 1)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Calculate the model output\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "def cross_entropy(T, Y):\n",
    "    return - (T * np.log(Y) + (1 - T) * np.log(1 - Y)).sum()\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(100):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy(T, Y))\n",
    "        \n",
    "    # Here is where you use what we derived above\n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.198603289\n",
      "1.59693764508\n",
      "1.15167791468\n",
      "0.922210128028\n",
      "0.776350382614\n",
      "0.673743481155\n",
      "0.59695800392\n",
      "0.537014206229\n",
      "0.488746327727\n",
      "0.448945277639\n",
      "0.415500488732\n",
      "0.386961130273\n",
      "0.36229396171\n",
      "0.340741467845\n",
      "0.321734632297\n",
      "Final w: [-3.0627943   2.82476522  5.37199592]\n"
     ]
    }
   ],
   "source": [
    "# Compare with closed form solution:\n",
    "w2 = np.array([0, 4, 4])\n",
    "# We're far away yet\n",
    "# More iterations?\n",
    "\n",
    "w = np.random.randn(D + 1)\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(150):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy(T, Y))\n",
    "        \n",
    "    # Here is where you use what we derived above\n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304.534078262\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Final w: [ -6.16616321  11.64989518  20.05007614]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Almost seems like a local minimum here eh,\n",
    "#    although it doesn't make sense because I'm pretty sure it's a convex\n",
    "#    objective.\n",
    "# Compare with closed form solution:\n",
    "w2 = np.array([0, 4, 4])\n",
    "# We're far away yet\n",
    "# More iterations?\n",
    "\n",
    "w = np.random.randn(D + 1)\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(150):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy(T, Y))\n",
    "        \n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.6052910996\n",
      "1.04732203569\n",
      "0.514008307598\n",
      "0.339789567136\n",
      "0.256415093052\n",
      "0.207187514235\n",
      "0.174585801322\n",
      "0.151417300646\n",
      "0.134149346571\n",
      "0.120828213424\n",
      "0.110279357124\n",
      "0.101751646517\n",
      "0.0947412661633\n",
      "0.0888972058543\n",
      "0.0839672191938\n",
      "0.0797653048518\n",
      "0.0761512980752\n",
      "0.0730176012487\n",
      "0.0702802911945\n",
      "0.0678730011559\n",
      "Final w: [-4.81795914  4.88905143  8.81262154]\n"
     ]
    }
   ],
   "source": [
    "# And what's with these nan errors?\n",
    "\n",
    "def cross_entropy2(T, Y):\n",
    "    E = 0\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E\n",
    "\n",
    "w = np.random.randn(D + 1)\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(200):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy2(T, Y))\n",
    "        \n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.9337472311\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Final w: [-4.48606932  4.79089743  8.61609692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Almost seems like a local minimum here eh,\n",
    "#    although it doesn't make sense because I'm pretty sure it's a convex\n",
    "#    objective.\n",
    "# Compare with closed form solution:\n",
    "w2 = np.array([0, 4, 4])\n",
    "# We're far away yet\n",
    "# More iterations?\n",
    "\n",
    "w = np.random.randn(D + 1)\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(150):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy(T, Y))\n",
    "        \n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "The nan problem:\n",
    "Clearly the problem is $np.log(0)$ somewhere in the problem based on the warning. \n",
    "\n",
    "In the vectorized cross_entropy function, I'm calculating $np.log(1 - Y)$ and $np.log(Y)$ on the entire vector $Y$, including potential predictions of 0 and 1, which will result in $nan$ with $np.log$. The non-vectorized implementation calculates $np.log(y_i)$ or  $np.log(y_i)$ only if $t_i == 1$ or $t_i == 0$ respectively. In both cases the operand is closer to 1 than 0. This problem can occur even with the non-vector function if for some reason the classifier is bad enough to predict 0 for a target = 1 or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.2226887599\n",
      "5.01351232036\n",
      "3.88079676329\n",
      "3.2862313922\n",
      "2.89681455625\n",
      "2.61430078632\n",
      "2.39668775813\n",
      "2.22223223881\n",
      "2.07827128374\n",
      "1.95681852289\n",
      "1.85253958716\n",
      "1.76171367578\n",
      "1.68165565833\n",
      "1.61037359406\n",
      "1.54635542276\n",
      "1.48843066962\n",
      "1.43567777587\n",
      "1.38736026271\n",
      "1.34288170995\n",
      "1.30175335088\n",
      "Final w: [-1.13437684  1.66408505  3.03458363] epochs = 200\n"
     ]
    }
   ],
   "source": [
    "# And what's with these nan errors?\n",
    "\n",
    "def cross_entropy2(T, Y):\n",
    "    E = 0\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(Y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - Y[i])\n",
    "    return E\n",
    "\n",
    "w = np.random.randn(D + 1)\n",
    "z = Xb.dot(w)\n",
    "Y = sigmoid(z)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "for i in range(epochs):\n",
    "    if i % 10 == 0:\n",
    "        print(cross_entropy2(T, Y))\n",
    "        \n",
    "    w += learning_rate * np.dot((T - Y).T, Xb)\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    "    \n",
    "print(\"Final w:\", w, \"epochs =\", epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

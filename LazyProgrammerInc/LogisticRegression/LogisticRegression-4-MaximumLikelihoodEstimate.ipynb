{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin Toss\n",
    "Coin with $p(H) = p$, $p(T) = 1 - p$\n",
    "\n",
    "In N = 10 tosses, we get 7H, 3T.\n",
    "\n",
    "You already know that the p is or should be 7/10. We can also calculate the maximum likelihood estimate to arrive at this number.\n",
    "\n",
    "We say, assume that the coin tosses were independent, and are identically distributed (since they come from the same coin).\n",
    "\n",
    "The likelihood of 7H and 3T is given by:\n",
    "\n",
    "$L = p^7 (1 - p)^3$\n",
    "\n",
    "$ l = log L = 7 log p + 3 log (1 - p) $\n",
    "\n",
    "$\\frac{\\partial l}{\\partial p} = \\frac{7}{p} + \\frac{3}{(1 - p)} = 0$\n",
    "\n",
    "Solve for p to get $p = 0.7 = p(H)$\n",
    "\n",
    "\n",
    "### Apply same concept logistic regression\n",
    "\n",
    "$P( y = 1 \\vert x ) = \\sigma (w^T x) = y$\n",
    "\n",
    "$ L = \\prod_{n=1}^N y_n^{t_n} (1 - y_n)^{1 - t_n} $\n",
    "\n",
    "$ l = log L = \\sum_n t_n * log y_n + (1 - t_n) * log (1 - y_n)$\n",
    "\n",
    "which is the same as the cross-entropy function we saw earlier, but with a negative sign.\n",
    "\n",
    "So, **Maximizing the log likelihood is the same as minimizing the cross-entropy error**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

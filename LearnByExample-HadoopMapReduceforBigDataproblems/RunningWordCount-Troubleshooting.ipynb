{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you have:\n",
    "\n",
    "1. WordCount.java  \n",
    "    ```\n",
    "    package me.nikhil;\n",
    "\n",
    "    import org.apache.hadoop.conf.Configuration;\n",
    "    import org.apache.hadoop.fs.Path;\n",
    "    import org.apache.hadoop.io.IntWritable;\n",
    "    import org.apache.hadoop.io.Text;\n",
    "    import org.apache.hadoop.mapreduce.Job;\n",
    "    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "    import java.io.IOException;\n",
    "\n",
    "    public class WordCount {\n",
    "\n",
    "        /* Requires input text file and output path */\n",
    "        public static void main(String args[]) throws Exception {\n",
    "\n",
    "            if (args.length != 2) {\n",
    "                System.err.println(\"Invalid Command\");\n",
    "                System.err.println(\"Usage: WordCount <input path> <output path>\");\n",
    "                System.exit(0);\n",
    "            }\n",
    "\n",
    "            Configuration conf = new Configuration();\n",
    "\n",
    "            Job job = new Job(conf, \"wordcount\");\n",
    "            job.setJarByClass(WordCount.class);\n",
    "            FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "            FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "            job.setMapperClass(WordCountMapper.class);\n",
    "            job.setReducerClass(WordCountReducer.class);\n",
    "            job.setOutputKeyClass(Text.class);  // For Reducer, and implicitly for Mapper also\n",
    "            job.setOutputValueClass(IntWritable.class);  // For Reducer, and implicitly for Mapper also\n",
    "\n",
    "            System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "\n",
    "2. WordCountMapper.java  \n",
    "    ```\n",
    "    package me.nikhil;\n",
    "\n",
    "    import org.apache.hadoop.io.IntWritable;\n",
    "    import org.apache.hadoop.io.LongWritable;\n",
    "    import org.apache.hadoop.io.Text;\n",
    "    import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "    import java.io.IOException;\n",
    "\n",
    "    public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "        /* \n",
    "            Generic class with the following data types to specify\n",
    "            <input key type, \n",
    "             input value type, \n",
    "             output key type, \n",
    "             output value type>\n",
    "        */\n",
    "\n",
    "        @Override\n",
    "        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            String line = value.toString();\n",
    "            for (String word : line.split(\" \")) {\n",
    "                if (word.length() > 0) {\n",
    "                    context.write(new Text(word), new IntWritable(1));\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "\n",
    "3. WordCountReducer.java  \n",
    "    ```\n",
    "    package me.nikhil;\n",
    "\n",
    "    import org.apache.hadoop.io.IntWritable;\n",
    "    import org.apache.hadoop.io.Text;\n",
    "    import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "    import java.io.IOException;\n",
    "\n",
    "    public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "\n",
    "        /* \n",
    "            Generic class with the following data types to specify\n",
    "            <input key type, \n",
    "             input value type, \n",
    "             output key type, \n",
    "             output value type>\n",
    "        */\n",
    "\n",
    "        @Override\n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "            int count = 0;\n",
    "            for (IntWritable value : values) {\n",
    "                count += value.get();\n",
    "            }\n",
    "\n",
    "            context.write(key, new IntWritable(count));\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "\n",
    "What do you do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, you create a jar**\n",
    "\n",
    "```\n",
    "$ ls -lrt\n",
    "total 40\n",
    "-rw-r--r--  1 genericuser  staff   752 Sep 26 12:40 WordCountReducer.java\n",
    "-rw-r--r--  1 genericuser  staff  3233 Sep 26 12:57 input.txt\n",
    "-rw-r--r--  1 genericuser  staff   835 Sep 26 13:52 WordCountMapper.java\n",
    "-rw-r--r--  1 genericuser  staff  1341 Sep 26 13:53 WordCount.java\n",
    "```\n",
    "\n",
    "```\n",
    "$ export CLASSPATH=\"/opt/hadoop-2.8.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.0.jar:/opt/hadoop-2.8.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.0.jar:/opt/hadoop-2.8.0/share/hadoop/common/hadoop-common-2.8.0.jar\"\n",
    "```\n",
    "\n",
    "```\n",
    "$ javac -d . WordCount.java WordCountReducer.java WordCountMapper.java\n",
    "```\n",
    "\n",
    "```\n",
    "$ ls -R me\n",
    "nikhil\n",
    "\n",
    "me/nikhil:\n",
    "WordCount.class        WordCountMapper.class  WordCountReducer.class\n",
    "```\n",
    "\n",
    "```\n",
    "$ printf \"Main-Class: me.nikhil.WordCount\\n\\n\" > Manifest.txt\n",
    "```\n",
    "\n",
    "```\n",
    "$ jar cfm wordcount.jar Manifest.txt me/nikhil/*.class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then make sure your input file is sent to hdfs**\n",
    "\n",
    "```\n",
    "$ hadoop fs -mkdir /test\n",
    "\n",
    "$ hadoop fs -put input.txt /test\n",
    "\n",
    "$ hadoop fs -ls /test\n",
    "17/09/26 14:42:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Found 1 items\n",
    "-rw-r--r--   1 genericuser supergroup       3233 2017-09-26 13:00 /test/input.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can ran hadoop**\n",
    "\n",
    "```\n",
    "$ hadoop jar wordcount.jar /test /wordcountoutput\n",
    "\n",
    "17/09/26 13:58:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "17/09/26 13:58:04 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
    "17/09/26 13:58:06 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:07 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:08 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:09 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:10 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:11 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:12 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:13 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:14 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:15 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
    "17/09/26 13:58:15 WARN ipc.Client: Failed to connect to server: 0.0.0.0/0.0.0.0:8032: retries get failed due to exceeded maximum allowed retries number: 10\n",
    "java.net.ConnectException: Connection refused\n",
    "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
    "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n",
    "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n",
    "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
    "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n",
    "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)\n",
    "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)\n",
    "\tat org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)\n",
    "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)\n",
    "\tat org.apache.hadoop.ipc.Client.call(Client.java:1373)\n",
    "\tat org.apache.hadoop.ipc.Client.call(Client.java:1337)\n",
    "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
    "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
    "\tat com.sun.proxy.$Proxy13.getNewApplication(Unknown Source)\n",
    "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(ApplicationClientProtocolPBClientImpl.java:258)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n",
    "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
    "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
    "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
    "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n",
    "\tat com.sun.proxy.$Proxy14.getNewApplication(Unknown Source)\n",
    "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNewApplication(YarnClientImpl.java:242)\n",
    "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createApplication(YarnClientImpl.java:250)\n",
    "\tat org.apache.hadoop.mapred.ResourceMgrDelegate.getNewJobID(ResourceMgrDelegate.java:193)\n",
    "\tat org.apache.hadoop.mapred.YARNRunner.getNewJobID(YARNRunner.java:241)\n",
    "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:155)\n",
    "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)\n",
    "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)\n",
    "\tat java.security.AccessController.doPrivileged(Native Method)\n",
    "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
    "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
    "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)\n",
    "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)\n",
    "\tat me.nikhil.WordCount.main(WordCount.java:35)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:234)\n",
    "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But of course it doesn't run the first time.** Because you want to make sure you have the following change made:\n",
    "\n",
    "```\n",
    "$ vim $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "\n",
    "<configuration>\n",
    "<!-- Site specific YARN configuration properties -->\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.resourcemanager.hostname</name>\n",
    "        <value>127.0.0.1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "Don't forget to stop and start yarn!\n",
    "\n",
    "```\n",
    "$ stop-yarn.sh\n",
    "$ start-yarn.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does it run now? Of course not...**\n",
    "\n",
    "```\n",
    "$ hadoop jar wordcount.jar /test /wordcountoutput\n",
    "\n",
    "17/09/26 14:49:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "17/09/26 14:49:13 INFO client.RMProxy: Connecting to ResourceManager at /127.0.0.1:8032\n",
    "17/09/26 14:49:14 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n",
    "17/09/26 14:49:15 INFO input.FileInputFormat: Total input files to process : 1\n",
    "17/09/26 14:49:15 INFO mapreduce.JobSubmitter: number of splits:1\n",
    "17/09/26 14:49:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1506462487747_0001\n",
    "17/09/26 14:49:16 INFO impl.YarnClientImpl: Submitted application application_1506462487747_0001\n",
    "17/09/26 14:49:16 INFO mapreduce.Job: The url to track the job: http://generics-air:8088/proxy/application_1506462487747_0001/\n",
    "17/09/26 14:49:16 INFO mapreduce.Job: Running job: job_1506462487747_0001\n",
    "```\n",
    "\n",
    "Gets stuck there forever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The configurations that finally worked are shown in ./config-files/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 120\r\n",
      "-rw-r--r--@  1 genericuser  staff   862 Sep 26 16:27 mapred-site.xml\r\n",
      "-rw-r--r--   1 genericuser  staff   673 Sep 26 16:27 .bashrc\r\n",
      "-rw-r--r--   1 genericuser  staff  3084 Sep 26 16:37 wordcount.jar\r\n",
      "-rwxr-xr-x   1 genericuser  staff    65 Sep 26 16:51 \u001b[31mrun-test2.sh\u001b[m\u001b[m*\r\n",
      "-rwxr-xr-x   1 genericuser  staff    48 Sep 26 16:55 \u001b[31mcopy-new-config.sh\u001b[m\u001b[m*\r\n",
      "-rwxr-xr-x   1 genericuser  staff   303 Sep 26 17:01 \u001b[31mrun-test1.sh\u001b[m\u001b[m*\r\n",
      "-rw-r--r--@  1 genericuser  staff   926 Sep 26 17:05 yarn-site.xml\r\n",
      "-rw-r--r--@  1 genericuser  staff  1391 Sep 26 17:25 hdfs-site.xml\r\n",
      "-rw-r--r--@  1 genericuser  staff   880 Sep 26 17:35 core-site.xml\r\n",
      "-rw-r--r--   1 genericuser  staff   411 Sep 26 17:40 hosts\r\n",
      "-rw-r--r--@  1 genericuser  staff  4718 Sep 26 17:55 hadoop-env.sh\r\n",
      "-rw-r--r--   1 genericuser  staff  6800 Sep 26 17:57 test1\r\n",
      "-rw-r--r--   1 genericuser  staff  3086 Sep 26 17:58 test2\r\n",
      "drwxr-xr-x  16 genericuser  staff   544 Sep 26 18:13 \u001b[34m.\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x  12 genericuser  staff   408 Sep 26 18:14 \u001b[34m.git\u001b[m\u001b[m/\r\n",
      "drwxr-xr-x   6 genericuser  staff   204 Sep 26 18:14 \u001b[34m..\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -lrta config-files/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
